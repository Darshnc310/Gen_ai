{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6dc2fd-a0b6-47b0-a079-90bb8ca1810f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fba0ecd4f1a45ba8bb03121bde3d1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7b60c7a6244017b9568f17a8916f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e609d933515f4b64b01ea646736e9c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196f7bca4f024329afc3a353c9682249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f7972b5b394c9ba32a9c514dcf60c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, you need to install the libraries:\n",
    "# pip install transformers torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 1. Load a pre-trained DECODER-ONLY model (GPT-2) and its tokenizer\n",
    "# \"gpt2\" is the original, small-but-powerful model from OpenAI\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to be the same as the end-of-sequence token\n",
    "# This is a common practice for decoder-only models\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Define our input prompt\n",
    "prompt_text = \"In a shocking finding, scientists today discovered a herd of unicorns\"\n",
    "\n",
    "# 3. Tokenize the input\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# 4. Generate the output text\n",
    "# We pass the tokenized prompt to the model.\n",
    "# The model will predict the next token, then the next, and so on.\n",
    "# - max_length: The total length (prompt + new text)\n",
    "# - num_beams: A technique for finding better-quality sentences\n",
    "# - early_stopping: Stops when the model generates an end-of-sequence token\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=50,       # Stop after 50 tokens\n",
    "        num_beams=5,         # Use beam search for higher quality\n",
    "        no_repeat_ngram_size=2, # Prevents repeating the same phrases\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# 5. Decode the output\n",
    "# This converts the model's output tokens back into a readable string.\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 6. Print the results\n",
    "print(f\"Prompt: {prompt_text}...\")\n",
    "print(\"---\")\n",
    "print(f\"LLM Generation:\\n{generated_text}\")\n",
    "\n",
    "# --- Example Output ---\n",
    "# Prompt: In a shocking finding, scientists today discovered a herd of unicorns...\n",
    "# ---\n",
    "# LLM Generation:\n",
    "# In a shocking finding, scientists today discovered a herd of unicorns\n",
    "# living in a remote, unexplored valley in the Andes Mountains.\n",
    "# The unicorns, which were previously thought to be mythical,\n",
    "# are believed to be the last of their kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914fa206-21f6-4455-9d1c-bba23d8882ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tallyai",
   "language": "python",
   "name": "tallyai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
